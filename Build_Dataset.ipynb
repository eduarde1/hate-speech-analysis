{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import emoji\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/eduarde/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eduarde/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eduarde/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/eduarde/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth ,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(hashtag_list):\n",
    "    tweets_amlo = pd.DataFrame(columns=['user', \"date_time\", \"likes\", \"tweet\"])\n",
    "    for hashtag in hashtag_list:\n",
    "        search_tw = tweepy.Cursor(api.search, q=f\"{hashtag} -filter:retweets\", tweet_mode='extended', count=2000).items(2000)\n",
    "        tweets = [[tweet.user.screen_name, str(tweet.created_at), tweet.user.favourites_count, tweet.full_text] \n",
    "                   for tweet in search_tw]\n",
    "        tweets_df = pd.DataFrame(data=tweets, columns=['user', \"date_time\", \"likes\", \"tweet\"])\n",
    "        tweets_amlo = pd.concat([tweets_amlo, tweets_df])\n",
    "    return tweets_amlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_list = ['#PresidenteRompaElPacto', '#AMLOEnfermoMental', '#AMLOEstamosContigo', '#AMLORenuncia', '#RenunciaAMLO', '#AMLOElFracasoPresidencial',\n",
    "                '#AMLOMasFuerteQueNunca', '#LopezFracasoPresidencial', '#MentirasDeCuarta', '#AMLOMuestraTusDatos', '#AMLOPresidenteDeLaSalud',\n",
    "                '#AMLOLujoDePresidente']\n",
    "\n",
    "tweets_amlo = get_tweets(hashtag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spanish_offensive_words') as f:\n",
    "    bad_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    allchars = [str for str in tweet]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    tweet = ' '.join([str for str in tweet.split() if not any(i in str for i in emoji_list)])\n",
    "    #tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    tweet = emoji.get_emoji_regexp().sub(u'', tweet)\n",
    "    tweet = ' '.join(' '.join(re.findall('[A-Z][^A-Z]*', word)) if word.startswith('#') else word for word in tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = tweet.strip()\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    clean_text = re.sub(r'\\W+', ' ', tweet).lower()\n",
    "    removed_numbers = ''.join(filter(lambda x: not x.isdigit(), clean_text))\n",
    "    removed_singles = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", removed_numbers)\n",
    "    removed_stopwords = [word for word in word_tokenize(removed_singles) if not word in stop_words]\n",
    "    filtered_text = ' '.join(removed_stopwords)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_amlo['tokenize_tweets'] = tweets_amlo['tweet'].map(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_amlo['offensive'] = tweets_amlo['tokenize_tweet'].str.contains('|'.join(bad_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_amlo['offensive'].replace({False: int(0), True: int(1)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12017\n",
       "1     1569\n",
       "Name: offensive, dtype: int64"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_amlo.offensive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_analysis_spanish import sentiment_analysis\n",
    "\n",
    "def get_sentiment_analysis(text):\n",
    "    sentiment = sentiment_analysis.SentimentAnalysisSpanish()\n",
    "    analysis = sentiment.sentiment(text)\n",
    "    return np.format_float_positional(analysis, trim='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_amlo[\"sentiment\"] = tweets_amlo[\"tokenized_tweet\"].map(lambda x: get_sentiment_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hate_speech(offensive, sentiment):\n",
    "    if (offensive == 1) and (sentiment < 0.5):\n",
    "        return 1\n",
    "    elif (offensive == 1) and (sentiment > 0.5):\n",
    "        return 0\n",
    "    elif (offensive == 0) and (sentiment < 0.1):\n",
    "        return 1\n",
    "    elif (offensive == 0) and (sentiment > 0.1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: get_hate_speech(offensive=x.offensive, sentiment=x.sentiment)\n",
    "tweets_amlo[\"hate_speech\"] = tweets_amlo.apply(func, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
